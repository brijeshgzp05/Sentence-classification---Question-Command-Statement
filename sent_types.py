# -*- coding: utf-8 -*-
"""sent_types.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pwxO_vehmKgBpUNzk-Y1Bui9L1vvZZ2p

# Sentence classification - Question, Command, Statement
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os

"""## Reading Data and displaying it.
I have processed it already. For you raw data is provided in data file1.csv
"""

import pandas as pd
os.chdir("/content/gdrive/My Drive")

df = pd.read_csv("processed_full_spaadia.csv")
df.head()

"""## Some more exloration"""

df.shape

print(df.isnull().sum())

"""## Changing categotical type to labels"""

types=df.type.unique()
dic={}
for i,type_ in enumerate(types):
    dic[type_]=i
labels=df.type.apply(lambda x:dic[x])

"""## Keras Import"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

"""## Splitting the data through sklearn"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.statement, df.type, test_size=0.3, random_state=42)

val_data = pd.concat([X_test,y_test],axis=1)
train_data = pd.concat([X_train,y_train],axis=1)

texts=df.statement

"""## Building Tokenizer"""

NUM_WORDS=10**5
tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n\'',lower=True)
tokenizer.fit_on_texts(texts)
sequences_train = tokenizer.texts_to_sequences(train_data.statement)
sequences_valid=tokenizer.texts_to_sequences(val_data.statement)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

"""## Padding"""

import numpy as np

max_length=50
trunc_type='post'

X_train = pad_sequences(sequences_train,maxlen=max_length, truncating = trunc_type)
X_val = pad_sequences(sequences_valid,maxlen=max_length, truncating = trunc_type)
y_train = to_categorical(np.asarray(labels[train_data.index]))
y_val = to_categorical(np.asarray(labels[val_data.index]))
print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)
print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)

"""## Constructing the model"""

from keras.optimizers import Adam
import keras

EMBEDDING_DIM=300
vocabulary_size=min(len(word_index)+1,NUM_WORDS)
model = keras.Sequential([
    keras.layers.Embedding(vocabulary_size,EMBEDDING_DIM,input_length=max_length),
    keras.layers.LSTM(16, activation='relu',return_sequences=True),
    keras.layers.Dropout(0.25),
    keras.layers.LSTM(8, activation='relu'),
    keras.layers.Dropout(0.25),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.25),
    keras.layers.Dense(3, activation='softmax')
])

"""## Compiling the model"""

adam = Adam(lr=1e-3)

model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['acc'])

"""## Summary"""



model.summary()

"""## Creating a callback function"""

class myCallback(keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs={}):
    if (logs.get('acc')>logs.get('val_acc')):
      print("\nOverfitting begins")
      self.model.stop_training=True

callbacks = myCallback()

"""## Fittng the data to model"""

epochs=10
verbose=1
batch_size=32
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
history=model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_val, y_val), callbacks=[callbacks])

"""## Testing on our own data"""

test_comments = [
    "This is a stupid example.",
    "This is another statement, perhaps this will trick the network",
    "I don't understand",
    "What's up?",
    "open the app",
    "This is another example",
    "Do what I tell you",
    "come over here and listen",
    "how do you know what to look for",
    "Remember how good the concert was?",
    "Who is the greatest basketball player of all time?",
    "Eat your cereal.",
    "Usually the prior sentence is not classified properly.",
    "Don't forget about your homework!",
    "Can the model identify a sentence without a question mark",
    "Everything speculated here is VC money and financial bubble with unrelaible financial values. Zomato, uber, paytm, flipkart throw discounts at the rate of losses. May be few can survive at the end. This hurts a lot for SMB too.",
    "I am trying to keep tabs on electric two-wheeler startup industry in India. Ather energy is emerging as a big name. Anyone knows how they are doing?",
    "generally a pretty intuitive way to accomplish a task. Want to trash an app Drag it to the trash Want to print a PDF",
    "Make sure ownership is clear and minimizing opportunities for such problematic outcomes in the second place",
    "Stop the video and walk away."
]

test_comments_category = [
    "statement",
    "statement",
    "statement",
    "question",
    "command",
    "statement",
    "command",
    "command",
    "question",
    "question",
    "question",
    "command",
    "statement",
    "command",
    "question",
    "statement",
    "question",
    "question",
    "statement",
    "command"
]

def prediction_single(sent):
    l = []
    l.append(sent)
    sequences_pred = tokenizer.texts_to_sequences(l)
    padded_pred = pad_sequences(sequences_pred,maxlen=max_length, truncating = trunc_type)
    pred = model.predict([padded_pred])
    pred = list(pred[0])
    ind = np.argmax(pred)
    if ind==0:
        return 'command'
    elif ind==1:
        return 'statement'
    else:
        return 'question'

for i in range(len(test_comments)):
    typ = prediction_single(test_comments[i])
    print(test_comments[i],'--------\nActual : ',test_comments_category[i],'--------prediction : ',typ)

model.save_weights("processed_full_spaadia_keras_model_new.h5")

import pickle

# saving
with open('processed_full_spaadia_keras_tokenizer_new.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""#### As you can see there is an imbalance in data i.e, of command type. But it works good on statements and questions. For commands we need more data."""

